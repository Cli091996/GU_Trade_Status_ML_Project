{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8e9657",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d98cbd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.388360Z",
     "start_time": "2024-04-09T00:56:55.372498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "#Need to clean further \n",
    "from Standardization import metric_normalizer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler,OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report  # Import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder,StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "#formats page\n",
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c529ee",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9702d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.432868Z",
     "start_time": "2024-04-09T00:56:57.392464Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/clivence/base_jupyter/Datadump/Model_Data/GU_Model_Data_V1_16k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f7a4c",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cab67ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.748834Z",
     "start_time": "2024-04-09T00:56:57.437636Z"
    }
   },
   "outputs": [],
   "source": [
    "#Filter out negative values - evetually need to research why this is happening \n",
    "df  = df[(df['2D Low in Pips'] > 0) | (df['2D Low in Pips'] > 0)] \n",
    "\n",
    "df = df[df['Action'] == 'Ultimate Action']\n",
    "df = df[df['Ticker'] == 'GBP/USD']\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "#Standardize the dataset\n",
    "df =  metric_normalizer(df)\n",
    "\n",
    "#Set date to datetime \n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "#Create Date Derived Features \n",
    "df['Trade_Week_Year'] = df['Date'].dt.isocalendar().week\n",
    "df['Trade_Week_Month'] = (df['Date'].dt.day -1)//7+1\n",
    "df['Trade_Day_Week'] = df['Date'].dt.weekday + 1\n",
    "\n",
    "#Create a new feature to identify the status of the previous trade \n",
    "df['Previous_Trade_Status'] = df['2D Trade Status'].shift(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02cf35d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T20:41:50.452097Z",
     "start_time": "2024-01-20T20:41:50.416223Z"
    }
   },
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98da383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.793872Z",
     "start_time": "2024-04-09T00:56:57.765611Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Subset Selection\n",
    "df = df[['open', 'high', 'low', 'close', 'volume','Trade_Week_Year','Trade_Week_Month','Trade_Day_Week',\n",
    "       'Day','Month','25EMA', '50EMA', '75EMA','100EMA', '125EMA',\n",
    "        'Trend Status', 'Spread','5075 Trend Status', '75100 Trend Status',\n",
    "       '100125 Trend Status', 'Order Type', \n",
    "        '%K', '%D', 'k_group', 'ADX', 'ADXR', 'slowk','slowd','CDL2CROWS', 'CDL3BLACKCROWS',\n",
    "        'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE',\n",
    "        'CDL3STARSINSOUTH', 'CDL3WHITESOLDIERS', 'CDLABANDONEDBABY', 'CDLADVANCEBLOCK',\n",
    "        'CDLBELTHOLD', 'CDLBREAKAWAY', 'CDLCLOSINGMARUBOZU', 'CDLCONCEALBABYSWALL',\n",
    "        'CDLCOUNTERATTACK', 'CDLDARKCLOUDCOVER', 'CDLDOJI', 'CDLDOJISTAR', 'CDLDRAGONFLYDOJI',\n",
    "        'CDLENGULFING', 'CDLEVENINGDOJISTAR', 'CDLEVENINGSTAR', 'CDLGAPSIDESIDEWHITE',\n",
    "        'CDLGRAVESTONEDOJI', 'CDLHAMMER', 'CDLHANGINGMAN', 'CDLHARAMI', 'CDLHARAMICROSS',\n",
    "        'CDLHIGHWAVE', 'CDLHIKKAKE', 'CDLHIKKAKEMOD', 'CDLHOMINGPIGEON', 'CDLIDENTICAL3CROWS',\n",
    "        'CDLINNECK', 'CDLINVERTEDHAMMER', 'CDLKICKING', 'CDLKICKINGBYLENGTH', 'CDLLADDERBOTTOM',\n",
    "        'CDLLONGLEGGEDDOJI', 'CDLLONGLINE', 'CDLMARUBOZU', 'CDLMATCHINGLOW', 'CDLMATHOLD',\n",
    "        'CDLMORNINGDOJISTAR', 'CDLMORNINGSTAR', 'CDLONNECK', 'CDLPIERCING', 'CDLRICKSHAWMAN',\n",
    "        'CDLRISEFALL3METHODS', 'CDLSEPARATINGLINES', 'CDLSHOOTINGSTAR', 'CDLSHORTLINE',\n",
    "        'CDLSPINNINGTOP', 'CDLSTALLEDPATTERN', 'CDLSTICKSANDWICH', 'CDLTAKURI', 'CDLTASUKIGAP',\n",
    "        'CDLTHRUSTING', 'CDLTRISTAR', 'CDLUNIQUE3RIVER', 'CDLUPSIDEGAP2CROWS', 'CDLXSIDEGAP3METHODS',\n",
    "        'candle_bullish_score','candle_bearish_score','Previous_Trade_Status','2D Trade Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31cfbd0f-5c09-49b4-8fad-f6d882992794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 386\n",
      "Test: 48\n",
      "Validation: 49\n"
     ]
    }
   ],
   "source": [
    "# set training, test, and validation set size \n",
    "total_len = len(df)\n",
    "training_size = int(total_len * .8)\n",
    "test_size = int(total_len * .1)\n",
    "validation_size = total_len - training_size - test_size\n",
    "\n",
    "# Split indices\n",
    "training_idx = df.index[:training_size]\n",
    "test_idx = df.index[training_size:training_size + test_size]\n",
    "validation_idx = df.index[-validation_size:]\n",
    "\n",
    "# Verify lengths\n",
    "assert len(training_idx) == training_size\n",
    "assert len(test_idx) == test_size\n",
    "assert len(validation_idx) == validation_size\n",
    "\n",
    "# Create training, test, and validation df \n",
    "training_df = df.loc[training_idx]\n",
    "test_df = df.loc[test_idx]\n",
    "validation_df = df.loc[validation_idx]\n",
    "\n",
    "# Print lengths of the three DataFrames\n",
    "print(f\"Training: {len(training_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")\n",
    "print(f\"Validation: {len(validation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba7e9593-a73f-45dd-8761-b779c0a4d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_df[['open', 'high', 'low', 'close', 'volume','Trade_Week_Year','Trade_Week_Month','Trade_Day_Week',\n",
    "       'Day','Month','25EMA', '50EMA', '75EMA','100EMA', '125EMA',\n",
    "        'Trend Status', 'Spread','5075 Trend Status', '75100 Trend Status',\n",
    "       '100125 Trend Status', 'Order Type', \n",
    "        '%K', '%D', 'k_group', 'ADX', 'ADXR', 'slowk','slowd','CDL2CROWS', 'CDL3BLACKCROWS',\n",
    "        'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE',\n",
    "        'CDL3STARSINSOUTH', 'CDL3WHITESOLDIERS', 'CDLABANDONEDBABY', 'CDLADVANCEBLOCK',\n",
    "        'CDLBELTHOLD', 'CDLBREAKAWAY', 'CDLCLOSINGMARUBOZU', 'CDLCONCEALBABYSWALL',\n",
    "        'CDLCOUNTERATTACK', 'CDLDARKCLOUDCOVER', 'CDLDOJI', 'CDLDOJISTAR', 'CDLDRAGONFLYDOJI',\n",
    "        'CDLENGULFING', 'CDLEVENINGDOJISTAR', 'CDLEVENINGSTAR', 'CDLGAPSIDESIDEWHITE',\n",
    "        'CDLGRAVESTONEDOJI', 'CDLHAMMER', 'CDLHANGINGMAN', 'CDLHARAMI', 'CDLHARAMICROSS',\n",
    "        'CDLHIGHWAVE', 'CDLHIKKAKE', 'CDLHIKKAKEMOD', 'CDLHOMINGPIGEON', 'CDLIDENTICAL3CROWS',\n",
    "        'CDLINNECK', 'CDLINVERTEDHAMMER', 'CDLKICKING', 'CDLKICKINGBYLENGTH', 'CDLLADDERBOTTOM',\n",
    "        'CDLLONGLEGGEDDOJI', 'CDLLONGLINE', 'CDLMARUBOZU', 'CDLMATCHINGLOW', 'CDLMATHOLD',\n",
    "        'CDLMORNINGDOJISTAR', 'CDLMORNINGSTAR', 'CDLONNECK', 'CDLPIERCING', 'CDLRICKSHAWMAN',\n",
    "        'CDLRISEFALL3METHODS', 'CDLSEPARATINGLINES', 'CDLSHOOTINGSTAR', 'CDLSHORTLINE',\n",
    "        'CDLSPINNINGTOP', 'CDLSTALLEDPATTERN', 'CDLSTICKSANDWICH', 'CDLTAKURI', 'CDLTASUKIGAP',\n",
    "        'CDLTHRUSTING', 'CDLTRISTAR', 'CDLUNIQUE3RIVER', 'CDLUPSIDEGAP2CROWS', 'CDLXSIDEGAP3METHODS',\n",
    "        'candle_bullish_score','candle_bearish_score','Previous_Trade_Status','2D Trade Status']]\n",
    "y_train = training_df['2D Trade Status']\n",
    "\n",
    "X_test = test_df[['open', 'high', 'low', 'close', 'volume','Trade_Week_Year','Trade_Week_Month','Trade_Day_Week',\n",
    "       'Day','Month','25EMA', '50EMA', '75EMA','100EMA', '125EMA',\n",
    "        'Trend Status', 'Spread','5075 Trend Status', '75100 Trend Status',\n",
    "       '100125 Trend Status', 'Order Type', \n",
    "        '%K', '%D', 'k_group', 'ADX', 'ADXR', 'slowk','slowd','CDL2CROWS', 'CDL3BLACKCROWS',\n",
    "        'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE',\n",
    "        'CDL3STARSINSOUTH', 'CDL3WHITESOLDIERS', 'CDLABANDONEDBABY', 'CDLADVANCEBLOCK',\n",
    "        'CDLBELTHOLD', 'CDLBREAKAWAY', 'CDLCLOSINGMARUBOZU', 'CDLCONCEALBABYSWALL',\n",
    "        'CDLCOUNTERATTACK', 'CDLDARKCLOUDCOVER', 'CDLDOJI', 'CDLDOJISTAR', 'CDLDRAGONFLYDOJI',\n",
    "        'CDLENGULFING', 'CDLEVENINGDOJISTAR', 'CDLEVENINGSTAR', 'CDLGAPSIDESIDEWHITE',\n",
    "        'CDLGRAVESTONEDOJI', 'CDLHAMMER', 'CDLHANGINGMAN', 'CDLHARAMI', 'CDLHARAMICROSS',\n",
    "        'CDLHIGHWAVE', 'CDLHIKKAKE', 'CDLHIKKAKEMOD', 'CDLHOMINGPIGEON', 'CDLIDENTICAL3CROWS',\n",
    "        'CDLINNECK', 'CDLINVERTEDHAMMER', 'CDLKICKING', 'CDLKICKINGBYLENGTH', 'CDLLADDERBOTTOM',\n",
    "        'CDLLONGLEGGEDDOJI', 'CDLLONGLINE', 'CDLMARUBOZU', 'CDLMATCHINGLOW', 'CDLMATHOLD',\n",
    "        'CDLMORNINGDOJISTAR', 'CDLMORNINGSTAR', 'CDLONNECK', 'CDLPIERCING', 'CDLRICKSHAWMAN',\n",
    "        'CDLRISEFALL3METHODS', 'CDLSEPARATINGLINES', 'CDLSHOOTINGSTAR', 'CDLSHORTLINE',\n",
    "        'CDLSPINNINGTOP', 'CDLSTALLEDPATTERN', 'CDLSTICKSANDWICH', 'CDLTAKURI', 'CDLTASUKIGAP',\n",
    "        'CDLTHRUSTING', 'CDLTRISTAR', 'CDLUNIQUE3RIVER', 'CDLUPSIDEGAP2CROWS', 'CDLXSIDEGAP3METHODS',\n",
    "        'candle_bullish_score','candle_bearish_score','Previous_Trade_Status','2D Trade Status']]\n",
    "y_test = test_df['2D Trade Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3299f2b-43e4-4625-a2b0-731bd790c0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93418d9f-c87f-4663-8f79-9a9932b166f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of columns to encode \n",
    "cat_cols_to_encode = ['Order Type']\n",
    "#Create a list of cols for ordinal encoding \n",
    "cat_cols_for_ordinal_encoding = ['Trend Status','5075 Trend Status', '75100 Trend Status',\n",
    "                    '100125 Trend Status','k_group']\n",
    "\n",
    "#Create a list of cols to scale \n",
    "num_cols_to_scale = ['volume']\n",
    "\n",
    "# Create Column Transformer with appropriate preprocessing for numerical and categorical data\n",
    "Preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "            ('scaler', MinMaxScaler())]), num_cols_to_scale),\n",
    "        ('cat_onehot', OneHotEncoder(drop='first', sparse_output=False), cat_cols_to_encode),\n",
    "        ('cat_ordinal', OrdinalEncoder(), cat_cols_for_ordinal_encoding)\n",
    "    ],\n",
    "    remainder='passthrough')\n",
    "\n",
    "# Preprocessor.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cb5dd87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.845107Z",
     "start_time": "2024-04-09T00:56:57.824067Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d32c2be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.873995Z",
     "start_time": "2024-04-09T00:56:57.849712Z"
    }
   },
   "outputs": [],
   "source": [
    "def enhanced_metrics( predictions,test=y_test):\n",
    "    '''This function calculates and returns various evaluation metrics based on predicted values and true labels.\n",
    "    Returns:\n",
    "- output: str\n",
    "  A string containing the calculated evaluation metrics.\n",
    "\n",
    "Metrics included:\n",
    "- Accuracy: The proportion of correctly classified samples.\n",
    "- F1 Score: The harmonic mean of precision and recall, indicating overall model performance.\n",
    "- Recall: The proportion of true positive samples correctly classified.\n",
    "- Precision: The proportion of predicted positive samples that are actually positive.\n",
    "    \n",
    "'''\n",
    "    Accuracy = accuracy_score(test, predictions)\n",
    "    f_score = f1_score(test, predictions)\n",
    "    recall_ = recall_score(test, predictions)\n",
    "    precision = precision_score(test, predictions)\n",
    "    Out_put = f\"Accuracy: {(Accuracy.round(decimals=3))*100}\\n\" + \\\n",
    "            f\"F1 Score: {(f_score.round(decimals=3))*100}\\n\" + \\\n",
    "            f\"Recall: {(recall_.round(decimals=3))*100}\\n\" + \\\n",
    "            f\"Precision: {(precision.round(decimals=3))*100}\" \n",
    "    return Out_put"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686faf5",
   "metadata": {},
   "source": [
    "### Base Models with No Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d28d085f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.908090Z",
     "start_time": "2024-04-09T00:56:57.878046Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clivence/base_jupyter/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_true' parameter of accuracy_score must be an array-like or a sparse matrix. Got 48 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m Base_LR_Model_Predictions \u001b[38;5;241m=\u001b[39m Base_LR_Model_Pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Display the metrics\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBase_LR_Model_Predictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base_jupyter/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[0;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n",
      "File \u001b[0;32m~/base_jupyter/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'y_true' parameter of accuracy_score must be an array-like or a sparse matrix. Got 48 instead."
     ]
    }
   ],
   "source": [
    "#Create PiPeline to be used in Model\n",
    "Base_LR_Model_Pipeline = Pipeline(steps=[\n",
    "    ('Preprocessor', Preprocessor),\n",
    "    ('RF_Model',LogisticRegression())])\n",
    "\n",
    "# Perform cross-validation\n",
    "Base_LR_Model_Pipeline.fit(X_train, y_train)\n",
    "Base_LR_Model_Predictions = Base_LR_Model_Pipeline.predict(X_test)\n",
    "\n",
    "# Display the metrics\n",
    "accuracy_score(test, Base_LR_Model_Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c9922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T23:54:24.956323Z",
     "start_time": "2024-04-08T23:54:24.936633Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a97c00ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.930201Z",
     "start_time": "2024-04-09T00:56:57.912624Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "# #Create Pipeline to be used in SVC Model\n",
    "# Base_SVC_Model_Pipeline = Pipeline(steps=[\n",
    "#     ('Preprocessor', Preprocessor),\n",
    "#     ('Feature_Selection', Selection),\n",
    "#     ('SVC_Model',SVC())])\n",
    "\n",
    "# # Perform cross-validation\n",
    "# SVC_CV_accuracy = cross_val_score(Base_SVC_Model_Pipeline, X, Y, cv=5, scoring='accuracy').mean()\n",
    "# SVC_CV_predictions = cross_val_predict(Base_SVC_Model_Pipeline, X, Y, cv=5)\n",
    "\n",
    "# # Calculate recall, precision, and F1 score for each fold\n",
    "# SVC_CV_recall = recall_score(Y, SVC_CV_predictions, average=None).mean()\n",
    "# SVC_CV_precision = precision_score(Y, SVC_CV_predictions, average=None).mean()\n",
    "# SVC_CV_f1_score = f1_score(Y, SVC_CV_predictions, average=None).mean()\n",
    "\n",
    "# # Display the metrics\n",
    "# print(\"Accuracy:\", SVC_CV_accuracy)\n",
    "# print(\"Recall:\", SVC_CV_recall)\n",
    "# print(\"Precision:\", SVC_CV_precision)\n",
    "# print(\"F1 Score:\", SVC_CV_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21c2371e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:56:57.953249Z",
     "start_time": "2024-04-09T00:56:57.943959Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Create Pipeline to be used in Descision Tree Model\n",
    "# Base_DT_Model_Pipeline = Pipeline(steps=[\n",
    "#     ('Preprocessor', Preprocessor),\n",
    "#     ('Feature_Selection', Selection),\n",
    "#     ('DT_Model',DecisionTreeClassifier())])\n",
    "\n",
    "# #Fit the training data \n",
    "# Base_DT_Model_Pipeline.fit(X_train, y_train)\n",
    "\n",
    "# #Make Predictions on the test set \n",
    "# Base_DT_Model_Predictions = Base_DT_Model_Pipeline.predict(X_test)\n",
    "\n",
    "# #Store individual metrics in a variable for later use \n",
    "# DT_Accuracy = (accuracy_score(y_test, Base_DT_Model_Predictions))*100\n",
    "# DT_f_score = (f1_score(y_test, Base_DT_Model_Predictions))*100\n",
    "# DT_recall_ = (recall_score(y_test, Base_DT_Model_Predictions))*100\n",
    "# DT_precision = (precision_score(y_test, Base_DT_Model_Predictions))*100\n",
    "\n",
    "# #Get Error Metrics of Random Forest Model\n",
    "# DT_Model_Metrics = enhanced_metrics(y_test, Base_DT_Model_Predictions)\n",
    "\n",
    "# #View Error Metric \n",
    "# print(DT_Model_Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5424f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T00:57:11.309858Z",
     "start_time": "2024-04-09T00:56:57.958725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5549319727891157\n",
      "Recall: 0.43522615535889875\n",
      "Precision: 0.42342342342342343\n",
      "F1 Score: 0.4272358732522545\n"
     ]
    }
   ],
   "source": [
    "# Create Model Pipeline\n",
    "Base_RF_Model_Pipeline = Pipeline(steps=[\n",
    "    ('Preprocessor', Preprocessor),\n",
    "    ('Feature_Selection', Selection),\n",
    "    ('RF_Model', RandomForestClassifier(random_state=42))  \n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "RF_CV_accuracy = cross_val_score(Base_RF_Model_Pipeline, X, Y, cv=10, scoring='accuracy').mean()\n",
    "RF_CV_predictions = cross_val_predict(Base_RF_Model_Pipeline, X, Y, cv=10)\n",
    "\n",
    "# Calculate recall, precision, and F1 score for each fold\n",
    "RF_CV_recall = recall_score(Y, RF_CV_predictions, average=None).mean()\n",
    "RF_CV_precision = precision_score(Y, RF_CV_predictions, average=None).mean()\n",
    "RF_CV_f1_score = f1_score(Y, RF_CV_predictions, average=None).mean()\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Accuracy:\", RF_CV_accuracy)\n",
    "print(\"Recall:\", RF_CV_recall)\n",
    "print(\"Precision:\", RF_CV_precision)\n",
    "print(\"F1 Score:\", RF_CV_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1d3cb-5a0e-4399-888f-52237a5383f9",
   "metadata": {},
   "source": [
    "### Grid Search For Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "541a4b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:54.105403Z",
     "start_time": "2024-04-09T00:57:11.312604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15552 candidates, totalling 77760 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "25920 fits failed out of a total of 77760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25920 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 402, in fit\n",
      "    raise ValueError(\n",
      "ValueError: `max_sample` cannot be set if `bootstrap=False`. Either switch to `bootstrap=True` or set `max_sample=None`.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/clivence/base_jupyter/venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.65625427 0.65885167 0.66924129 ...        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'RF_Model__bootstrap': False, 'RF_Model__ccp_alpha': 0.0, 'RF_Model__criterion': 'entropy', 'RF_Model__max_depth': 20, 'RF_Model__max_features': 'sqrt', 'RF_Model__max_samples': None, 'RF_Model__min_samples_leaf': 2, 'RF_Model__min_samples_split': 2, 'RF_Model__n_estimators': 100}\n",
      "Best score found:  0.7031784005468216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to use for the grid search\n",
    "param_grid = {\n",
    "    'RF_Model__n_estimators': [25,50, 100, 200],  # Number of trees in the forest\n",
    "    'RF_Model__criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'RF_Model__max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "    'RF_Model__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'RF_Model__min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'RF_Model__max_features': ['sqrt', 'log2', None],  # Number of features to consider when looking for the best split\n",
    "    # Note: For max_features, None means that max_features=n_features\n",
    "    'RF_Model__bootstrap': [True, False],  # Whether bootstrap samples are used when building trees\n",
    "    'RF_Model__ccp_alpha': [0.0, 0.01, 0.1],  # Complexity parameter used for Minimal Cost-Complexity Pruning\n",
    "    'RF_Model__max_samples': [None, 0.5, 0.75]  # If bootstrap is True, the number of samples to draw from X to train each base estimator\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=Base_RF_Model_Pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score found: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8810c-91dc-44e2-8e98-2be563c15054",
   "metadata": {},
   "source": [
    "### Grid Search Metrics For Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f9a5171-4938-4a32-96eb-e216c56045ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5569727891156462\n",
      "Recall: 0.45467674532940017\n",
      "Precision: 0.4513094190140845\n",
      "F1 Score: 0.45243897270723854\n"
     ]
    }
   ],
   "source": [
    "# Create Model Pipeline\n",
    "Base_RF_Model_Pipeline = Pipeline(steps=[\n",
    "    ('Preprocessor', Preprocessor),\n",
    "    ('Feature_Selection', Selection),\n",
    "    ('RF_Model', RandomForestClassifier(random_state=42,\n",
    "                                       \n",
    "                                       ccp_alpha=  0.0,\n",
    "                                       criterion ='entropy',\n",
    "                                       max_depth= 20,\n",
    "                                       max_features= 'sqrt',\n",
    "                                       min_samples_leaf= 2,\n",
    "                                       min_samples_split= 2,\n",
    "                                       n_estimators= 100,\n",
    "                                       bootstrap= False,\n",
    "                                       max_samples= None,\n",
    "))  \n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "RF_CV_accuracy = cross_val_score(Base_RF_Model_Pipeline, X, Y, cv=10, scoring='accuracy').mean()\n",
    "RF_CV_predictions = cross_val_predict(Base_RF_Model_Pipeline, X, Y, cv=10)\n",
    "\n",
    "# Calculate recall, precision, and F1 score for each fold\n",
    "RF_CV_recall = recall_score(Y, RF_CV_predictions, average=None).mean()\n",
    "RF_CV_precision = precision_score(Y, RF_CV_predictions, average=None).mean()\n",
    "RF_CV_f1_score = f1_score(Y, RF_CV_predictions, average=None).mean()\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Accuracy:\", RF_CV_accuracy)\n",
    "print(\"Recall:\", RF_CV_recall)\n",
    "print(\"Precision:\", RF_CV_precision)\n",
    "print(\"F1 Score:\", RF_CV_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41c1a77c-419c-42b9-8f7d-069926615950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mForestClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    A random forest classifier.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    A random forest is a meta estimator that fits a number of decision tree\u001b[0m\n",
       "\u001b[0;34m    classifiers on various sub-samples of the dataset and uses averaging to\u001b[0m\n",
       "\u001b[0;34m    improve the predictive accuracy and control over-fitting.\u001b[0m\n",
       "\u001b[0;34m    Trees in the forest use the best split strategy, i.e. equivalent to passing\u001b[0m\n",
       "\u001b[0;34m    `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\u001b[0m\n",
       "\u001b[0;34m    The sub-sample size is controlled with the `max_samples` parameter if\u001b[0m\n",
       "\u001b[0;34m    `bootstrap=True` (default), otherwise the whole dataset is used to build\u001b[0m\n",
       "\u001b[0;34m    each tree.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    For a comparison between tree-based ensemble models see the example\u001b[0m\n",
       "\u001b[0;34m    :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Read more in the :ref:`User Guide <forest>`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Parameters\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    n_estimators : int, default=100\u001b[0m\n",
       "\u001b[0;34m        The number of trees in the forest.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 0.22\u001b[0m\n",
       "\u001b[0;34m           The default value of ``n_estimators`` changed from 10 to 100\u001b[0m\n",
       "\u001b[0;34m           in 0.22.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\u001b[0m\n",
       "\u001b[0;34m        The function to measure the quality of a split. Supported criteria are\u001b[0m\n",
       "\u001b[0;34m        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\u001b[0m\n",
       "\u001b[0;34m        Shannon information gain, see :ref:`tree_mathematical_formulation`.\u001b[0m\n",
       "\u001b[0;34m        Note: This parameter is tree-specific.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_depth : int, default=None\u001b[0m\n",
       "\u001b[0;34m        The maximum depth of the tree. If None, then nodes are expanded until\u001b[0m\n",
       "\u001b[0;34m        all leaves are pure or until all leaves contain less than\u001b[0m\n",
       "\u001b[0;34m        min_samples_split samples.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_samples_split : int or float, default=2\u001b[0m\n",
       "\u001b[0;34m        The minimum number of samples required to split an internal node:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If int, then consider `min_samples_split` as the minimum number.\u001b[0m\n",
       "\u001b[0;34m        - If float, then `min_samples_split` is a fraction and\u001b[0m\n",
       "\u001b[0;34m          `ceil(min_samples_split * n_samples)` are the minimum\u001b[0m\n",
       "\u001b[0;34m          number of samples for each split.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
       "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_samples_leaf : int or float, default=1\u001b[0m\n",
       "\u001b[0;34m        The minimum number of samples required to be at a leaf node.\u001b[0m\n",
       "\u001b[0;34m        A split point at any depth will only be considered if it leaves at\u001b[0m\n",
       "\u001b[0;34m        least ``min_samples_leaf`` training samples in each of the left and\u001b[0m\n",
       "\u001b[0;34m        right branches.  This may have the effect of smoothing the model,\u001b[0m\n",
       "\u001b[0;34m        especially in regression.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If int, then consider `min_samples_leaf` as the minimum number.\u001b[0m\n",
       "\u001b[0;34m        - If float, then `min_samples_leaf` is a fraction and\u001b[0m\n",
       "\u001b[0;34m          `ceil(min_samples_leaf * n_samples)` are the minimum\u001b[0m\n",
       "\u001b[0;34m          number of samples for each node.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 0.18\u001b[0m\n",
       "\u001b[0;34m           Added float values for fractions.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_weight_fraction_leaf : float, default=0.0\u001b[0m\n",
       "\u001b[0;34m        The minimum weighted fraction of the sum total of weights (of all\u001b[0m\n",
       "\u001b[0;34m        the input samples) required to be at a leaf node. Samples have\u001b[0m\n",
       "\u001b[0;34m        equal weight when sample_weight is not provided.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\u001b[0m\n",
       "\u001b[0;34m        The number of features to consider when looking for the best split:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If int, then consider `max_features` features at each split.\u001b[0m\n",
       "\u001b[0;34m        - If float, then `max_features` is a fraction and\u001b[0m\n",
       "\u001b[0;34m          `max(1, int(max_features * n_features_in_))` features are considered at each\u001b[0m\n",
       "\u001b[0;34m          split.\u001b[0m\n",
       "\u001b[0;34m        - If \"sqrt\", then `max_features=sqrt(n_features)`.\u001b[0m\n",
       "\u001b[0;34m        - If \"log2\", then `max_features=log2(n_features)`.\u001b[0m\n",
       "\u001b[0;34m        - If None, then `max_features=n_features`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionchanged:: 1.1\u001b[0m\n",
       "\u001b[0;34m            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Note: the search for a split does not stop until at least one\u001b[0m\n",
       "\u001b[0;34m        valid partition of the node samples is found, even if it requires to\u001b[0m\n",
       "\u001b[0;34m        effectively inspect more than ``max_features`` features.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_leaf_nodes : int, default=None\u001b[0m\n",
       "\u001b[0;34m        Grow trees with ``max_leaf_nodes`` in best-first fashion.\u001b[0m\n",
       "\u001b[0;34m        Best nodes are defined as relative reduction in impurity.\u001b[0m\n",
       "\u001b[0;34m        If None then unlimited number of leaf nodes.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    min_impurity_decrease : float, default=0.0\u001b[0m\n",
       "\u001b[0;34m        A node will be split if this split induces a decrease of the impurity\u001b[0m\n",
       "\u001b[0;34m        greater than or equal to this value.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The weighted impurity decrease equation is the following::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            N_t / N * (impurity - N_t_R / N_t * right_impurity\u001b[0m\n",
       "\u001b[0;34m                                - N_t_L / N_t * left_impurity)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        where ``N`` is the total number of samples, ``N_t`` is the number of\u001b[0m\n",
       "\u001b[0;34m        samples at the current node, ``N_t_L`` is the number of samples in the\u001b[0m\n",
       "\u001b[0;34m        left child, and ``N_t_R`` is the number of samples in the right child.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\u001b[0m\n",
       "\u001b[0;34m        if ``sample_weight`` is passed.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.19\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    bootstrap : bool, default=True\u001b[0m\n",
       "\u001b[0;34m        Whether bootstrap samples are used when building trees. If False, the\u001b[0m\n",
       "\u001b[0;34m        whole dataset is used to build each tree.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    oob_score : bool or callable, default=False\u001b[0m\n",
       "\u001b[0;34m        Whether to use out-of-bag samples to estimate the generalization score.\u001b[0m\n",
       "\u001b[0;34m        By default, :func:`~sklearn.metrics.accuracy_score` is used.\u001b[0m\n",
       "\u001b[0;34m        Provide a callable with signature `metric(y_true, y_pred)` to use a\u001b[0m\n",
       "\u001b[0;34m        custom metric. Only available if `bootstrap=True`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_jobs : int, default=None\u001b[0m\n",
       "\u001b[0;34m        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\u001b[0m\n",
       "\u001b[0;34m        :meth:`decision_path` and :meth:`apply` are all parallelized over the\u001b[0m\n",
       "\u001b[0;34m        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\u001b[0m\n",
       "\u001b[0;34m        context. ``-1`` means using all processors. See :term:`Glossary\u001b[0m\n",
       "\u001b[0;34m        <n_jobs>` for more details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    random_state : int, RandomState instance or None, default=None\u001b[0m\n",
       "\u001b[0;34m        Controls both the randomness of the bootstrapping of the samples used\u001b[0m\n",
       "\u001b[0;34m        when building trees (if ``bootstrap=True``) and the sampling of the\u001b[0m\n",
       "\u001b[0;34m        features to consider when looking for the best split at each node\u001b[0m\n",
       "\u001b[0;34m        (if ``max_features < n_features``).\u001b[0m\n",
       "\u001b[0;34m        See :term:`Glossary <random_state>` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    verbose : int, default=0\u001b[0m\n",
       "\u001b[0;34m        Controls the verbosity when fitting and predicting.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    warm_start : bool, default=False\u001b[0m\n",
       "\u001b[0;34m        When set to ``True``, reuse the solution of the previous call to fit\u001b[0m\n",
       "\u001b[0;34m        and add more estimators to the ensemble, otherwise, just fit a whole\u001b[0m\n",
       "\u001b[0;34m        new forest. See :term:`Glossary <warm_start>` and\u001b[0m\n",
       "\u001b[0;34m        :ref:`gradient_boosting_warm_start` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\u001b[0m\n",
       "\u001b[0;34m            default=None\u001b[0m\n",
       "\u001b[0;34m        Weights associated with classes in the form ``{class_label: weight}``.\u001b[0m\n",
       "\u001b[0;34m        If not given, all classes are supposed to have weight one. For\u001b[0m\n",
       "\u001b[0;34m        multi-output problems, a list of dicts can be provided in the same\u001b[0m\n",
       "\u001b[0;34m        order as the columns of y.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Note that for multioutput (including multilabel) weights should be\u001b[0m\n",
       "\u001b[0;34m        defined for each class of every column in its own dict. For example,\u001b[0m\n",
       "\u001b[0;34m        for four-class multilabel classification weights should be\u001b[0m\n",
       "\u001b[0;34m        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\u001b[0m\n",
       "\u001b[0;34m        [{1:1}, {2:5}, {3:1}, {4:1}].\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The \"balanced\" mode uses the values of y to automatically adjust\u001b[0m\n",
       "\u001b[0;34m        weights inversely proportional to class frequencies in the input data\u001b[0m\n",
       "\u001b[0;34m        as ``n_samples / (n_classes * np.bincount(y))``\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The \"balanced_subsample\" mode is the same as \"balanced\" except that\u001b[0m\n",
       "\u001b[0;34m        weights are computed based on the bootstrap sample for every tree\u001b[0m\n",
       "\u001b[0;34m        grown.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        For multi-output, the weights of each column of y will be multiplied.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Note that these weights will be multiplied with sample_weight (passed\u001b[0m\n",
       "\u001b[0;34m        through the fit method) if sample_weight is specified.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    ccp_alpha : non-negative float, default=0.0\u001b[0m\n",
       "\u001b[0;34m        Complexity parameter used for Minimal Cost-Complexity Pruning. The\u001b[0m\n",
       "\u001b[0;34m        subtree with the largest cost complexity that is smaller than\u001b[0m\n",
       "\u001b[0;34m        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\u001b[0m\n",
       "\u001b[0;34m        :ref:`minimal_cost_complexity_pruning` for details.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    max_samples : int or float, default=None\u001b[0m\n",
       "\u001b[0;34m        If bootstrap is True, the number of samples to draw from X\u001b[0m\n",
       "\u001b[0;34m        to train each base estimator.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - If None (default), then draw `X.shape[0]` samples.\u001b[0m\n",
       "\u001b[0;34m        - If int, then draw `max_samples` samples.\u001b[0m\n",
       "\u001b[0;34m        - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\u001b[0m\n",
       "\u001b[0;34m          `max_samples` should be in the interval `(0.0, 1.0]`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.22\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    monotonic_cst : array-like of int of shape (n_features), default=None\u001b[0m\n",
       "\u001b[0;34m        Indicates the monotonicity constraint to enforce on each feature.\u001b[0m\n",
       "\u001b[0;34m          - 1: monotonic increase\u001b[0m\n",
       "\u001b[0;34m          - 0: no constraint\u001b[0m\n",
       "\u001b[0;34m          - -1: monotonic decrease\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        If monotonic_cst is None, no constraints are applied.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Monotonicity constraints are not supported for:\u001b[0m\n",
       "\u001b[0;34m          - multiclass classifications (i.e. when `n_classes > 2`),\u001b[0m\n",
       "\u001b[0;34m          - multioutput classifications (i.e. when `n_outputs_ > 1`),\u001b[0m\n",
       "\u001b[0;34m          - classifications trained on data with missing values.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        The constraints hold over the probability of the positive class.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 1.4\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Attributes\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\u001b[0m\n",
       "\u001b[0;34m        The child estimator template used to create the collection of fitted\u001b[0m\n",
       "\u001b[0;34m        sub-estimators.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 1.2\u001b[0m\n",
       "\u001b[0;34m           `base_estimator_` was renamed to `estimator_`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    estimators_ : list of DecisionTreeClassifier\u001b[0m\n",
       "\u001b[0;34m        The collection of fitted sub-estimators.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    classes_ : ndarray of shape (n_classes,) or a list of such arrays\u001b[0m\n",
       "\u001b[0;34m        The classes labels (single output problem), or a list of arrays of\u001b[0m\n",
       "\u001b[0;34m        class labels (multi-output problem).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_classes_ : int or list\u001b[0m\n",
       "\u001b[0;34m        The number of classes (single output problem), or a list containing the\u001b[0m\n",
       "\u001b[0;34m        number of classes for each output (multi-output problem).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_features_in_ : int\u001b[0m\n",
       "\u001b[0;34m        Number of features seen during :term:`fit`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 0.24\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[0m\n",
       "\u001b[0;34m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[0m\n",
       "\u001b[0;34m        has feature names that are all strings.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 1.0\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    n_outputs_ : int\u001b[0m\n",
       "\u001b[0;34m        The number of outputs when ``fit`` is performed.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    feature_importances_ : ndarray of shape (n_features,)\u001b[0m\n",
       "\u001b[0;34m        The impurity-based feature importances.\u001b[0m\n",
       "\u001b[0;34m        The higher, the more important the feature.\u001b[0m\n",
       "\u001b[0;34m        The importance of a feature is computed as the (normalized)\u001b[0m\n",
       "\u001b[0;34m        total reduction of the criterion brought by that feature.  It is also\u001b[0m\n",
       "\u001b[0;34m        known as the Gini importance.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Warning: impurity-based feature importances can be misleading for\u001b[0m\n",
       "\u001b[0;34m        high cardinality features (many unique values). See\u001b[0m\n",
       "\u001b[0;34m        :func:`sklearn.inspection.permutation_importance` as an alternative.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    oob_score_ : float\u001b[0m\n",
       "\u001b[0;34m        Score of the training dataset obtained using an out-of-bag estimate.\u001b[0m\n",
       "\u001b[0;34m        This attribute exists only when ``oob_score`` is True.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\u001b[0m\n",
       "\u001b[0;34m            (n_samples, n_classes, n_outputs)\u001b[0m\n",
       "\u001b[0;34m        Decision function computed with out-of-bag estimate on the training\u001b[0m\n",
       "\u001b[0;34m        set. If n_estimators is small it might be possible that a data point\u001b[0m\n",
       "\u001b[0;34m        was never left out during the bootstrap. In this case,\u001b[0m\n",
       "\u001b[0;34m        `oob_decision_function_` might contain NaN. This attribute exists\u001b[0m\n",
       "\u001b[0;34m        only when ``oob_score`` is True.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    estimators_samples_ : list of arrays\u001b[0m\n",
       "\u001b[0;34m        The subset of drawn samples (i.e., the in-bag samples) for each base\u001b[0m\n",
       "\u001b[0;34m        estimator. Each subset is defined by an array of the indices selected.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        .. versionadded:: 1.4\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    See Also\u001b[0m\n",
       "\u001b[0;34m    --------\u001b[0m\n",
       "\u001b[0;34m    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\u001b[0m\n",
       "\u001b[0;34m    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\u001b[0m\n",
       "\u001b[0;34m        tree classifiers.\u001b[0m\n",
       "\u001b[0;34m    sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\u001b[0m\n",
       "\u001b[0;34m        Boosting Classification Tree, very fast for big datasets (n_samples >=\u001b[0m\n",
       "\u001b[0;34m        10_000).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Notes\u001b[0m\n",
       "\u001b[0;34m    -----\u001b[0m\n",
       "\u001b[0;34m    The default values for the parameters controlling the size of the trees\u001b[0m\n",
       "\u001b[0;34m    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\u001b[0m\n",
       "\u001b[0;34m    unpruned trees which can potentially be very large on some data sets. To\u001b[0m\n",
       "\u001b[0;34m    reduce memory consumption, the complexity and size of the trees should be\u001b[0m\n",
       "\u001b[0;34m    controlled by setting those parameter values.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The features are always randomly permuted at each split. Therefore,\u001b[0m\n",
       "\u001b[0;34m    the best found split may vary, even with the same training data,\u001b[0m\n",
       "\u001b[0;34m    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\u001b[0m\n",
       "\u001b[0;34m    of the criterion is identical for several splits enumerated during the\u001b[0m\n",
       "\u001b[0;34m    search of the best split. To obtain a deterministic behaviour during\u001b[0m\n",
       "\u001b[0;34m    fitting, ``random_state`` has to be fixed.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    References\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Examples\u001b[0m\n",
       "\u001b[0;34m    --------\u001b[0m\n",
       "\u001b[0;34m    >>> from sklearn.ensemble import RandomForestClassifier\u001b[0m\n",
       "\u001b[0;34m    >>> from sklearn.datasets import make_classification\u001b[0m\n",
       "\u001b[0;34m    >>> X, y = make_classification(n_samples=1000, n_features=4,\u001b[0m\n",
       "\u001b[0;34m    ...                            n_informative=2, n_redundant=0,\u001b[0m\n",
       "\u001b[0;34m    ...                            random_state=0, shuffle=False)\u001b[0m\n",
       "\u001b[0;34m    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\u001b[0m\n",
       "\u001b[0;34m    >>> clf.fit(X, y)\u001b[0m\n",
       "\u001b[0;34m    RandomForestClassifier(...)\u001b[0m\n",
       "\u001b[0;34m    >>> print(clf.predict([[0, 0, 0, 0]]))\u001b[0m\n",
       "\u001b[0;34m    [1]\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mForestClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"class_weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mStrOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"balanced_subsample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"splitter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sqrt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mestimator_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_depth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_split\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_samples_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_weight_fraction_leaf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"max_leaf_nodes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"min_impurity_decrease\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"random_state\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"ccp_alpha\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"monotonic_cst\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic_cst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mccp_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccp_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/base_jupyter/venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24154e0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:54.138024Z",
     "start_time": "2024-04-09T07:23:54.120568Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define the parameter grid, prefixing each parameter with the name of the pipeline step followed by two underscores\n",
    "# param_grid = {\n",
    "#     'GBC_Model__loss': ['deviance', 'exponential'],\n",
    "#     'GBC_Model__learning_rate': [0.01, 0.1, 0.5],\n",
    "#     'GBC_Model__n_estimators': [50, 100, 200],\n",
    "#     'GBC_Model__subsample': [0.5, 0.75, 1.0],\n",
    "#     'GBC_Model__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "#     'GBC_Model__min_samples_split': [2, 5, 10],\n",
    "#     'GBC_Model__min_samples_leaf': [1, 2, 4],\n",
    "#     'GBC_Model__max_depth': [3, 5, 10],\n",
    "#     'GBC_Model__min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "#     'GBC_Model__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "#     'GBC_Model__warm_start': [True, False]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=Base_GBC_Model_Pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# print(\"Best parameters found:\")\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b803889a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:54.758126Z",
     "start_time": "2024-04-09T07:23:54.143728Z"
    }
   },
   "outputs": [],
   "source": [
    "??RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02008b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:54.766765Z",
     "start_time": "2024-04-09T07:23:54.761061Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid to use for the grid search\n",
    "# param_grid = {\n",
    "#     'RF_Model__n_estimators': [10, 20,30,50,75, 100,150, 200],  # Number of trees in the forest\n",
    "#     'RF_Model__criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "#     'RF_Model__max_depth': [None,5, 10,15, 20],  # Maximum depth of the tree\n",
    "#     'RF_Model__min_samples_split': [2, 5, 10, 15,],  # Minimum number of samples required to split an internal node\n",
    "#     'RF_Model__min_samples_leaf': [1,3, 5],  # Minimum number of samples required to be at a leaf node\n",
    "#     'RF_Model__max_features': ['sqrt', 'log2', None],  # Number of features to consider when looking for the best split\n",
    "#     # Note: For max_features, None means that max_features=n_features\n",
    "#     'RF_Model__bootstrap': [True, False],  # Whether bootstrap samples are used when building trees\n",
    "#     'RF_Model__ccp_alpha': [0.0, 0.01, 0.1],  # Complexity parameter used for Minimal Cost-Complexity Pruning\n",
    "#     'RF_Model__max_samples': [None,0.25, 0.5, 0.75]  # If bootstrap is True, the number of samples to draw from X to train each base estimator\n",
    "# }\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=Base_RF_Model_Pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# # Assuming X_train and y_train are already defined\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best score found: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f602b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83217de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ccfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c364750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebef77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5adcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f58be31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:54.792102Z",
     "start_time": "2024-04-09T07:23:54.770051Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Create Pipeline to be used in Gradient Boosting Classfier Model\n",
    "# Base_GBC_Model_Pipeline = Pipeline(steps=[\n",
    "#     ('Preprocessor', Preprocessor),\n",
    "#     ('GBC_Model',GradientBoostingClassifier())])\n",
    "\n",
    "# # Perform cross-validation\n",
    "# GBC_CV_accuracy = cross_val_score(Base_GBC_Model_Pipeline, X, Y, cv=5, scoring='accuracy').mean()\n",
    "# GBC_CV_predictions = cross_val_predict(Base_GBC_Model_Pipeline, X, Y, cv=5)\n",
    "\n",
    "# # Calculate recall, precision, and F1 score for each fold\n",
    "# GBC_CV_recall = recall_score(Y, GBC_CV_predictions, average=None).mean()\n",
    "# GBC_CV_precision = precision_score(Y, GBC_CV_predictions, average=None).mean()\n",
    "# GBC_CV_f1_score = f1_score(Y, GBC_CV_predictions, average=None).mean()\n",
    "\n",
    "# # Display the metrics\n",
    "# print(\"Accuracy:\", GBC_CV_accuracy)\n",
    "# print(\"Recall:\", GBC_CV_recall)\n",
    "# print(\"Precision:\", GBC_CV_precision)\n",
    "# print(\"F1 Score:\", GBC_CV_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70c163ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:56.743105Z",
     "start_time": "2024-04-09T07:23:54.795405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5629939862542955\n",
      "Recall: 0.4670907079646017\n",
      "Precision: 0.4654887218045113\n",
      "F1 Score: 0.466013109566001\n"
     ]
    }
   ],
   "source": [
    "#Create Pipeline to be used in Gradient Boosting Classfier Model\n",
    "Base_GBC_Model_Pipeline = Pipeline(steps=[\n",
    "    ('Preprocessor', Preprocessor),\n",
    "    ('GBC_Model',GradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample=.75,\n",
    "    criterion='friedman_mse',\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_depth=5,\n",
    "    min_impurity_decrease=0.1,\n",
    "    random_state=42,\n",
    "    max_features='sqrt',\n",
    "    verbose=0,\n",
    "    max_leaf_nodes=None,\n",
    "    warm_start=False,\n",
    "    validation_fraction=0.1,\n",
    "   ))])\n",
    "\n",
    "# Perform cross-validation\n",
    "GBC_CV_accuracy = cross_val_score(Base_GBC_Model_Pipeline, X, Y, cv=5, scoring='accuracy').mean()\n",
    "GBC_CV_predictions = cross_val_predict(Base_GBC_Model_Pipeline, X, Y, cv=5)\n",
    "\n",
    "# Calculate recall, precision, and F1 score for each fold\n",
    "GBC_CV_recall = recall_score(Y, GBC_CV_predictions, average=None).mean()\n",
    "GBC_CV_precision = precision_score(Y, GBC_CV_predictions, average=None).mean()\n",
    "GBC_CV_f1_score = f1_score(Y, GBC_CV_predictions, average=None).mean()\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Accuracy:\", GBC_CV_accuracy)\n",
    "print(\"Recall:\", GBC_CV_recall)\n",
    "print(\"Precision:\", GBC_CV_precision)\n",
    "print(\"F1 Score:\", GBC_CV_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a1d6570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:57.812674Z",
     "start_time": "2024-04-09T07:23:56.759340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold CV Results:\n",
      "Accuracy: 0.6833118556701031\n",
      "Recall: 0.8967954345917472\n",
      "Precision: 0.7205529209160536\n",
      "F1 Score: 0.7990249871289621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "# Define StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = ['accuracy', 'recall', 'precision', 'f1']\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(Base_GBC_Model_Pipeline, X, Y, cv=skf, scoring=scoring)\n",
    "\n",
    "# Calculate the mean of each metric\n",
    "accuracy_mean = cv_results['test_accuracy'].mean()\n",
    "recall_mean = cv_results['test_recall'].mean()\n",
    "precision_mean = cv_results['test_precision'].mean()\n",
    "f1_mean = cv_results['test_f1'].mean()\n",
    "\n",
    "print(\"StratifiedKFold CV Results:\")\n",
    "print(f\"Accuracy: {accuracy_mean}\")\n",
    "print(f\"Recall: {recall_mean}\")\n",
    "print(f\"Precision: {precision_mean}\")\n",
    "print(f\"F1 Score: {f1_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24227caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:58.869519Z",
     "start_time": "2024-04-09T07:23:57.816028Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Define ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(Base_GBC_Model_Pipeline, X, Y, cv=ss, scoring=scoring)\n",
    "\n",
    "# Calculate the mean of each metric\n",
    "accuracy_mean = cv_results['test_accuracy'].mean()\n",
    "recall_mean = cv_results['test_recall'].mean()\n",
    "precision_mean = cv_results['test_precision'].mean()\n",
    "f1_mean = cv_results['test_f1'].mean()\n",
    "\n",
    "print(\"ShuffleSplit CV Results:\")\n",
    "print(f\"Accuracy: {accuracy_mean}\")\n",
    "print(f\"Recall: {recall_mean}\")\n",
    "print(f\"Precision: {precision_mean}\")\n",
    "print(f\"F1 Score: {f1_mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3463dfdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:58.889248Z",
     "start_time": "2024-04-09T07:23:58.883957Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8caf0a3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:58.927870Z",
     "start_time": "2024-04-09T07:23:58.893189Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Given values\n",
    "# values = [0.01, 0.1, 0.5]\n",
    "\n",
    "# # Using list comprehension with numpy.array to recreate the list\n",
    "# list_from_numpy = [np.array(values)[i] for i in range(len(values))]\n",
    "\n",
    "# print(list_from_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc32a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T23:54:47.114040Z",
     "start_time": "2024-04-08T23:54:47.088402Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0b20031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:58.961274Z",
     "start_time": "2024-04-09T07:23:58.938982Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Define the parameter grid, prefixing each parameter with the name of the pipeline step followed by two underscores\n",
    "# param_grid = {\n",
    "#     'GBC_Model__loss': ['deviance', 'exponential'],\n",
    "#     'GBC_Model__learning_rate': [0.01, 0.1, 0.5],\n",
    "#     'GBC_Model__n_estimators': [50, 100, 200],\n",
    "#     'GBC_Model__subsample': [0.5, 0.75, 1.0],\n",
    "#     'GBC_Model__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "#     'GBC_Model__min_samples_split': [2, 5, 10],\n",
    "#     'GBC_Model__min_samples_leaf': [1, 2, 4],\n",
    "#     'GBC_Model__max_depth': [3, 5, 10],\n",
    "#     'GBC_Model__min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "#     'GBC_Model__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "#     'GBC_Model__warm_start': [True, False]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=Base_GBC_Model_Pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# print(\"Best parameters found:\")\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dc7c9ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:58.993280Z",
     "start_time": "2024-04-09T07:23:58.966731Z"
    }
   },
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(Base_GBC_Model_Pipeline, params, cv=5, scoring='accuracy')\n",
    "# grid.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe24d099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.012861Z",
     "start_time": "2024-04-09T07:23:59.006952Z"
    }
   },
   "outputs": [],
   "source": [
    "# ??GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2133339",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.037063Z",
     "start_time": "2024-04-09T07:23:59.022266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Best parameters found:\n",
    "# {'GBC_Model__criterion': 'friedman_mse',\n",
    "#  'GBC_Model__learning_rate': 0.1, \n",
    "#  'GBC_Model__loss': 'exponential',\n",
    "#  'GBC_Model__max_depth': 5,\n",
    "#  'GBC_Model__max_features': 'sqrt', \n",
    "#  'GBC_Model__min_impurity_decrease': 0.1,\n",
    "#  'GBC_Model__min_samples_leaf': 4, \n",
    "#  'GBC_Model__min_samples_split': 10,\n",
    "#  'GBC_Model__n_estimators': 100,\n",
    "#  'GBC_Model__subsample': 0.75,\n",
    "#  'GBC_Model__warm_start': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843a00f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.563409Z",
     "start_time": "2024-04-09T07:23:59.042675Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create PiPeline to be used in Model\n",
    "Base_ADAC_Model_Pipeline = Pipeline(steps=[\n",
    "    ('Preprocessor', Preprocessor),\n",
    "    ('RF_Model',AdaBoostClassifier())])\n",
    "\n",
    "#Fit the training data \n",
    "Base_ADAC_Model_Pipeline.fit(X_train, y_train)\n",
    "\n",
    "#Make Predictions on the test set \n",
    "Base_ADAC_Model_Predictions = Base_ADAC_Model_Pipeline.predict(X_test)\n",
    "\n",
    "#Store individual metrics in a variable for later use \n",
    "ADAC_Accuracy = (accuracy_score(y_test, Base_ADAC_Model_Predictions))*100\n",
    "ADAC_f_score = (f1_score(y_test, Base_ADAC_Model_Predictions))*100\n",
    "ADAC_recall_ = (recall_score(y_test, Base_ADAC_Model_Predictions))*100\n",
    "ADAC_precision = (precision_score(y_test, Base_ADAC_Model_Predictions))*100\n",
    "\n",
    "\n",
    "#Get Error Metrics of Random Forest Model\n",
    "ADAC_Model_Metrics = enhanced_metrics(y_test, Base_ADAC_Model_Predictions)\n",
    "\n",
    "#View Error Metric \n",
    "print(ADAC_Model_Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b0a1b",
   "metadata": {},
   "source": [
    "### Base Model Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14efeb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.569618Z",
     "start_time": "2024-04-09T07:23:59.569563Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a dataframe to score metric scores\n",
    "Classifier_Scores = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614aaf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.579151Z",
     "start_time": "2024-04-09T07:23:59.579108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to store these values\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Logistic Regression': [LR_Accuracy, LR_f_score, LR_recall_, LR_precision],\n",
    "    'Descision Tree' : [DT_Accuracy, DT_f_score, DT_recall_, DT_precision],\n",
    "    'Support Vector Machine' : [SVC_Accuracy, SVC_f_score, SVC_recall_, SVC_precision],\n",
    "    'Random Forest' : [RF_Accuracy, RF_f_score, RF_recall_, RF_precision],\n",
    "    'Gradient Boosting' : [GBC_Accuracy, GBC_f_score, GBC_recall_, GBC_precision],\n",
    "    'ADA Boost ' : [ADAC_Accuracy, ADAC_f_score, ADAC_recall_, ADAC_precision],\n",
    "}, index=['Accuracy','F1 Score', 'Recall', 'Precision'])\n",
    "\n",
    "# Transpose the DataFrame \n",
    "metrics_df = metrics_df.transpose()\n",
    "\n",
    "#rename the index \n",
    "metrics_df.index.name='Model Name'\n",
    "\n",
    "metrics_df.reset_index(inplace=True)\n",
    "\n",
    "# # Display the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9c425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.588070Z",
     "start_time": "2024-04-09T07:23:59.588026Z"
    }
   },
   "outputs": [],
   "source": [
    "#Sort the df based on each metric \n",
    "accuracy_sorted_df = metrics_df.sort_values('Accuracy',ascending=False)\n",
    "f1_sorted_df = metrics_df.sort_values('F1 Score',ascending=False)\n",
    "recall_sorted_df = metrics_df.sort_values('Recall',ascending=False)\n",
    "precision_sorted_df = metrics_df.sort_values('Precision',ascending=False)\n",
    "\n",
    "#Create Figure \n",
    "fig, axs= plt.subplots(1,4, figsize=(24,10))\n",
    "\n",
    "fig.suptitle(\"Base Model Performance\",fontsize=16)\n",
    "\n",
    "#Plot Accuracy in AXS[0,0]\n",
    "bar_accuracy = axs[0].bar(accuracy_sorted_df['Model Name'], accuracy_sorted_df['Accuracy'])\n",
    "axs[0].set_title('Accuracy By Classifiers')\n",
    "axs[0].set_ylabel('Percentage %')\n",
    "axs[0].set_xticklabels(accuracy_sorted_df['Model Name'], rotation=45)\n",
    "for bar in bar_accuracy:\n",
    "    height = bar.get_height()\n",
    "    axs[0].text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "#Plot Precision in AXS[1[]\n",
    "bar_precision = axs[1].bar(precision_sorted_df['Model Name'], precision_sorted_df['Precision'])\n",
    "axs[1].set_title('Precision By Classifiers')\n",
    "axs[1].set_ylabel('Percentage %')\n",
    "axs[1].set_xticklabels(precision_sorted_df['Model Name'], rotation=45)\n",
    "for bar in bar_precision:\n",
    "    height = bar.get_height()\n",
    "    axs[1].text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "#Plot Recall in AXS[2]\n",
    "bar_recall = axs[2].bar(recall_sorted_df['Model Name'], recall_sorted_df['Recall'])\n",
    "axs[2].set_title('Recall By Classifiers')\n",
    "axs[2].set_ylabel('Percentage %')\n",
    "axs[2].set_xticklabels(recall_sorted_df['Model Name'], rotation=45)\n",
    "for bar in bar_recall:\n",
    "    height = bar.get_height()\n",
    "    axs[2].text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "#Plot F1 Score in AXS[3]\n",
    "bar_fscore = axs[3].bar(f1_sorted_df['Model Name'], f1_sorted_df['F1 Score'])\n",
    "axs[3].set_title('F1 Score By Classifiers')\n",
    "axs[3].set_ylabel('Percentage %')\n",
    "axs[3].set_xticklabels(f1_sorted_df['Model Name'], rotation=45)\n",
    "for bar in bar_fscore:\n",
    "    height = bar.get_height()\n",
    "    axs[3].text(bar.get_x() + bar.get_width() / 2, height, f'{height:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f373a1",
   "metadata": {},
   "source": [
    "### There might be an issue between the Recall and Precision value of Logistic Regression - do not delete this until investigate further "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ecb6ee",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc5fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T07:23:59.592428Z",
     "start_time": "2024-04-09T07:23:59.592390Z"
    }
   },
   "outputs": [],
   "source": [
    "#Retrived the Feature Selection Step \n",
    "feature_selection = Base_RF_Model_Pipeline.named_steps['Feature_Selection']\n",
    "\n",
    "#Retrieve the trained model \n",
    "trained_RF_Model = Base_RF_Model_Pipeline.named_steps['RF_Model']\n",
    "\n",
    "feature_importances = trained_RF_Model.feature_importances_\n",
    "\n",
    "# Get the indices of selected features\n",
    "selected_feature_indices = feature_selection.get_support(indices=True)\n",
    "\n",
    "# Extract feature names\n",
    "all_feature_names = Preprocessor.get_feature_names_out()\n",
    "selected_feature_names = [all_feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "# Combine feature names with their importances\n",
    "features_and_importances = zip(selected_feature_names, feature_importances)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_features_and_importances = sorted(features_and_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# Create a DataFrame from the features and importances\n",
    "feature_importance_df = pd.DataFrame(sorted_features_and_importances, columns=['Feature', 'Importance'])\n",
    "\n",
    "# Display the DataFrame\n",
    "feature_importance_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fe12b",
   "metadata": {},
   "source": [
    "### Reference: Concepts To Understand \n",
    "\n",
    "#### Accuracy: \n",
    "                - Measures the overall correctness of the classifier. It's calculated by dividing the number of correctly classifier samples by the total number of samples. \n",
    "                - Accuracy tells us how often the classifier correctly predicted whether a trade is a successful or not compared to the total number of trades.\n",
    "                - An Accuracy of .76 means that classifier correctly predicted the trade status for approximately 76% of the time. \n",
    "                \n",
    "#### Recall:\n",
    "             - Also known as sensitivity or true positive, measures  the ability of the classifier to correctly identify all positive instances. \n",
    "             - In the context of a trade status classifier, recall tells us how often the classifier correctly identified out of all successful trades.\n",
    "             - A recall of approximately .795 means that the classifier correctly identified 79.5% of the successful trades. \n",
    "#### Precision \n",
    "              - Measures the ability of the classifier to correctly identify only relevant instances among all instances it has classified as positive. \n",
    "              - In the context of a trade status classifier, precision measures the proportion of correctly predicted successful trades out of all successful trades.\n",
    "              - A precision of approximately .933 means that about 93.3% of the trades predicted as successful by the classifier were actually successful. \n",
    "#### F1 Score\n",
    "                - Is a metric that combines precision and recall into a single value - it is the harmonic mean between precision and recall. \n",
    "                - Precision measures the proportion of correctly predicted positive cases among all predicted cases.\n",
    "                - While Recall, measures the proportion of correctly predicted positive cases among all actual positive cases.\n",
    "                - F1 Score provides a balance between precision and recall, it is especially useful when there is an uneven class distribution.\n",
    "                - In the context of a trade status classifier, an F1 Score of .859 indicate a good balance between precision and recall predicting whether trades were successful or not. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
